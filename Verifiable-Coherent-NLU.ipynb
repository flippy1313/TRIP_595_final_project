{"cells":[{"cell_type":"markdown","metadata":{"id":"XL9AU7zTgP9n"},"source":["### **Toward Consistent, Verifiable, and Coherent Commonsense Reasoning in Large LMs**\n","\n","This notebook provides source code for our two papers in Findings of EMNLP 2021:\n","\n","\n","1.  Shane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Y. Chai (2021). *Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding.* Findings of EMNLP 2021.\n","2.   Shane Storks and Joyce Y. Chai (2021). *Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers.* Findings of EMNLP 2021.\n","\n","*If you have any questions or problems, please open an issue on our [GitHub repo](https://github.com/sled-group/Verifiable-Coherent-NLU) or email Shane Storks.*\n","\n","***First, configure the execution mode by selecting a few settings (expand cell if needed):***\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"ct4cd2_TFYDk"},"source":["   0. (Colab only) Insert the path in your Google Drive to the folder where this notebook is located."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700954583724,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"Vq9-dXJXFWh3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/nliang/595project\n"]}],"source":["import os\n","import logging\n","logging.disable(logging.WARNING )\n","DRIVE_PATH = os.getcwd()\n","print(DRIVE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"yxzL0hhHAHzN"},"source":["1.   Model type (choose from BERT large, RoBERTa large, RoBERTa large + MNLI, DeBERTa base, and DeBERTa large).\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1700954584780,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"FFNe6vlTaHsP","outputId":"3b51c6f6-a382-417f-fb2f-5cf3d4ffc2fd"},"outputs":[],"source":["# mode = 'bert' # BERT large\n","mode = 'roberta' # RoBERTa large\n","# mode = 'roberta_mnli' # RoBERTa large pre-trained on MNLI\n","# mode = 'deberta' # DeBERTa base for training on TRIP\n","# mode = 'deberta_large' # DeBERTa large for training on CE and ART"]},{"cell_type":"markdown","metadata":{"id":"KbJ-XeY1aCpD"},"source":["2.   Name of the task we want to train or evaluate on. Set `debug` to `True` to run quick training/evaluation jobs on only a small amount of data."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700954584781,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"iAQGu6JMa-o6"},"outputs":[],"source":["task_name = 'trip'\n","\n","debug = False"]},{"cell_type":"markdown","metadata":{"id":"aoWKXfQBd435"},"source":["3.   (If training models) Training batch size, learning rate, and maximum number of epochs. Settings for results in the paper are provided as examples."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1700954584783,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"UyFFcZtkeKwT"},"outputs":[],"source":["config_batch_size = 1\n","config_lr = 1e-5 # Selected learning rate for best RoBERTa-based model in TRIP paper\n","config_epochs = 10"]},{"cell_type":"markdown","metadata":{"id":"hH9a70CTaGpG"},"source":["4.   (For training TRIP models only) Configure the loss weighting scheme for training models here. We provide the 4 modes from the paper as examples.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1700954584784,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"UvQEiNBSACak"},"outputs":[],"source":["# Loss weights for (attributes, preconditions, effects, conflicts, story choices)\n","if task_name != 'trip':\n","  print(\"We do not need a loss weighting scheme for %s dataset. Ignoring this cell.\" % task_name)\n","# loss_weights = [0.0, 0.4, 0.4, 0.1, 0.1] # \"All losses\"\n","loss_weights = [0.0, 0.4, 0.4, 0.2, 0.0] # \"Omit story choice loss\"\n","#loss_weights = [0.0, 0.4, 0.4, 0.0, 0.2] # \"Omit conflict detection loss\"\n","# loss_weights = [0.0, 0.0, 0.0, 0.5, 0.5] # \"Omit state classification losses\""]},{"cell_type":"markdown","metadata":{"id":"3GA5VS3Sfgfz"},"source":["  **For more configuration options, scroll down to the Train Models > Configure Hyperparameters cell for the task you're working on.**"]},{"cell_type":"markdown","metadata":{"id":"qqvj34KhLL0k"},"source":["# Setup\n","Run this block every time when starting up the notebook. It will get Colab ready, preprocess the data, and load model packages and classes we'll need later. May take several minutes to run for the first time.\n","\n","**If you get a `ModuleNotFoundError` for the `www` code base, try the following:**\n","\n","\n","1.   Ensure the DRIVE_PATH is set properly above.\n","2.   (Colab only) Verify that this notebook has access to your Google Drive (click the folder icon on the left and then the Google Drive icon).\n","2.   Try to restart the runtime and refresh your browser window.\n","2.   (Colab only) If the problem persists, revoke access to Google Drive and re-enable it.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xm7qzKnAnbU9"},"source":["## Colab Setup\n","\n","Enable auto reloading of code libraries from Google Drive, set up connection to Google Drive, and import some packages. üîå"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10102,"status":"ok","timestamp":1700954599385,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"P3dNQeNNnkHD","outputId":"53ad9d7a-c72d-4aef-e6b2-4597c1dfd20d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nliang/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import load_entry_point\n","Requirement already satisfied: jsonlines in /home/nliang/.local/lib/python3.8/site-packages (4.0.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from jsonlines) (19.3.0)\n","True\n"]}],"source":["import os\n","import json\n","import sys\n","import torch\n","import random\n","import numpy as np\n","import spacy\n","!pip install jsonlines\n","print(torch.cuda.is_available())\n","sys.path.append(DRIVE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"zQLB_Y-wSsfk"},"source":["## Model Setup\n","\n","Next, we'll load up the transformer model, tokenizer, etc. ‚è≥"]},{"cell_type":"markdown","metadata":{"id":"FoY37xIF-oP7"},"source":["### Install HuggingFace transformers and other dependencies"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25304,"status":"ok","timestamp":1700954628140,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"_rp3vUVjT9I4","outputId":"d4adc135-56c1-4336-a504-d3384db0bb51"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import load_entry_point\n","Requirement already up-to-date: wheel in /home/nliang/.local/lib/python3.8/site-packages (0.42.0)\n","Requirement already up-to-date: setuptools in /home/nliang/.local/lib/python3.8/site-packages (69.0.2)\n","Collecting pip\n","  Using cached pip-23.3.1-py3-none-any.whl (2.1 MB)\n","Installing collected packages: pip\n","Successfully installed pip-23.3.1\n","/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import load_entry_point\n","Requirement already satisfied: transformers in /home/nliang/.local/lib/python3.8/site-packages (4.35.2)\n","Requirement already satisfied: filelock in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (2023.10.3)\n","Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /home/nliang/.local/lib/python3.8/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/nliang/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nliang/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n","/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import load_entry_point\n","Requirement already satisfied: sentencepiece in /home/nliang/.local/lib/python3.8/site-packages (0.1.99)\n","/usr/bin/pip3:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import load_entry_point\n","Requirement already satisfied: torch==1.11.0 in /home/nliang/.local/lib/python3.8/site-packages (1.11.0+cu113)\n","Requirement already satisfied: torchvision==0.12.0 in /home/nliang/.local/lib/python3.8/site-packages (0.12.0)\n","Requirement already satisfied: torchaudio==0.11.0 in /home/nliang/.local/lib/python3.8/site-packages (0.11.0)\n","Requirement already satisfied: typing-extensions in /home/nliang/.local/lib/python3.8/site-packages (from torch==1.11.0) (4.8.0)\n","Requirement already satisfied: numpy in /home/nliang/.local/lib/python3.8/site-packages (from torchvision==0.12.0) (1.24.4)\n","Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision==0.12.0) (2.22.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision==0.12.0) (7.0.0)\n"]}],"source":["!pip install wheel setuptools pip --upgrade\n","!pip install 'transformers'\n","!pip install sentencepiece\n","!pip3 install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0"]},{"cell_type":"markdown","metadata":{"id":"Z4LFuLhzAa2j"},"source":["### Get Model Components"]},{"cell_type":"markdown","metadata":{"id":"2ZhhgV9c__TU"},"source":["Specify which model parameters from transformers we want to use:"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":162,"status":"ok","timestamp":1700954638227,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"uA6XunCb_gd9"},"outputs":[{"data":{"text/plain":["'roberta-large'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["if task_name in ['trip', 'ce']:\n","  multiple_choice = False\n","elif task_name == 'art':\n","  multiple_choice = True\n","else:\n","  raise ValueError(\"Task name should be set to 'trip', 'ce', or 'art' in the first cell of the notebook!\")\n","\n","if mode == 'bert':\n","  model_name = 'bert-large-uncased'\n","elif mode == 'roberta':\n","  model_name = 'roberta-large'\n","elif mode == 'roberta_mnli':\n","  model_name = 'roberta-large-mnli'\n","model_name"]},{"cell_type":"markdown","metadata":{"id":"hgFIl1MJ-185"},"source":["Load the tokenizer:"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3568,"status":"ok","timestamp":1700954675101,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"etkxf75f-9Gj"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/nliang/595project/cache\n"]}],"source":["from transformers import BertTokenizer, RobertaTokenizer, DebertaTokenizer, AlbertTokenizer, T5Tokenizer, GPT2Tokenizer\n","#from DeBERTa import deberta\n","print(os.path.join(DRIVE_PATH, 'cache'))\n","if mode in ['bert']:\n","  tokenizer_class = BertTokenizer\n","elif mode in ['roberta', 'roberta_mnli']:\n","  tokenizer_class = RobertaTokenizer\n","elif mode in ['deberta', 'deberta_large']:\n","  tokenizer_class = DebertaTokenizer\n","\n","tokenizer = tokenizer_class.from_pretrained(model_name,\n","                                                do_lower_case = False,\n","                                                cache_dir=os.path.join(DRIVE_PATH, 'cache'))"]},{"cell_type":"markdown","metadata":{"id":"d0iYZG6bBGIf"},"source":["Load the model and optimizer:\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":860,"status":"ok","timestamp":1700954681115,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"1PxFghcDBPm_"},"outputs":[],"source":["from transformers import BertForSequenceClassification, RobertaForSequenceClassification, DebertaForSequenceClassification, AlbertForSequenceClassification, AdamW\n","from transformers import BertForMultipleChoice, RobertaForMultipleChoice, AlbertForMultipleChoice, DebertaModel\n","from transformers import BertModel, RobertaModel, AlbertModel, DebertaModel, T5Model, T5EncoderModel, GPT2Model\n","from transformers import RobertaForMaskedLM\n","from transformers import BertConfig, RobertaConfig, DebertaConfig, AlbertConfig, T5Config, GPT2Config\n","from www.model.transformers_ext import DebertaForMultipleChoice\n","from torch.optim import Adam\n","if not multiple_choice:\n","  if mode == 'bert':\n","    model_class = BertForSequenceClassification\n","    config_class = BertConfig\n","    emb_class = BertModel\n","  elif mode in ['roberta', 'roberta_mnli']:\n","    model_class = RobertaForSequenceClassification\n","    config_class = RobertaConfig\n","    emb_class = RobertaModel\n","    lm_class = RobertaForMaskedLM\n","  elif mode in ['deberta', 'deberta_large']:\n","    model_class = DebertaForSequenceClassification\n","    config_class = DebertaConfig\n","    emb_class = DebertaModel\n","else:\n","  if mode == 'bert':\n","    model_class = BertForMultipleChoice\n","    config_class = BertConfig\n","    emb_class = BertModel\n","  elif mode in ['roberta', 'roberta_mnli']:\n","    model_class = RobertaForMultipleChoice\n","    config_class = RobertaConfig\n","    emb_class = RobertaModel\n","    lm_class = RobertaForMaskedLM\n","  elif mode in ['deberta', 'deberta_large']:\n","    model_class = DebertaForMultipleChoice\n","    config_class = DebertaConfig\n","    emb_class = DebertaModel"]},{"cell_type":"markdown","metadata":{"id":"QzFnAVtuUmpQ"},"source":["## Data Setup\n","\n","Preprocess the dataset."]},{"cell_type":"markdown","metadata":{"id":"lKY0hTEgnQgB"},"source":["### Preprocessing\n","\n","Construct the dataset from the .txt files collected from AMT. Save a backup copy in Drive."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4497,"status":"ok","timestamp":1700954691562,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"aE-LOkJ4nWuu","outputId":"975fe2bd-97bb-4229-979d-db917facf8c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preprocessed examples:\n","{\n","  story_id: \n","    13,\n","  worker_id: \n","    A32W24TWSWXW,\n","  type: \n","    None,\n","  idx: \n","    None,\n","  aug: \n","    False,\n","  actor: \n","    John,\n","  location: \n","    kitchen,\n","  objects: \n","    cabinet, counter, knife, pan, potato, pizza,\n","  sentences: \n","    [\n","      John was getting the snacks ready for the party.\n","      John opened the cabinet, took out a pan and put it on the counter.\n","      John opened the fridge and got out the pizza.\n","      John put the pizza on the pan and put them into the oven.\n","      John took a knife and cut the hot pizza in eight slices.\n","    ],\n","  length: \n","    5,\n","  example_id: \n","    13,\n","  plausible: \n","    True,\n","  breakpoint: \n","    -1,\n","  confl_sents: \n","    [],\n","  confl_pairs: \n","    [],\n","  states: \n","    [\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['knife', 2], ['slices', 2], ['hot pizza', 0]], 'exist': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'clean': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'power': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'functional': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'pieces': [['knife', 0], ['slices', 0], ['hot pizza', 4]], 'wet': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'open': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'temperature': [['knife', 0], ['slices', 0], ['hot pizza', 2]], 'solid': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'contain': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'running': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'moveable': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'mixed': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'edible': [['knife', 0], ['slices', 0], ['hot pizza', 0]]}\n","    ],\n","}\n","\n","\n","{\n","  story_id: \n","    13,\n","  worker_id: \n","    A32W24TWSWXW,\n","  type: \n","    cloze,\n","  idx: \n","    0,\n","  aug: \n","    False,\n","  actor: \n","    John,\n","  location: \n","    kitchen,\n","  objects: \n","    cabinet, counter, knife, pan, potato, pizza,\n","  sentences: \n","    [\n","      John was getting the snacks ready for the party.\n","      John opened the cabinet, took out a pan and put it on the counter.\n","      John opened the fridge and got out the pizza.\n","      John put the pizza on the pan and put them into the oven.\n","      John called the pizza joint to deliver a pizza.\n","    ],\n","  length: \n","    5,\n","  example_id: \n","    13-C0,\n","  plausible: \n","    False,\n","  breakpoint: \n","    4,\n","  confl_sents: \n","    [\n","      2\n","      3\n","    ],\n","  confl_pairs: \n","    [\n","      [2, 4]\n","      [3, 4]\n","    ],\n","  states: \n","    [\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['pizza', 0]], 'exist': [['pizza', 4]], 'clean': [['pizza', 0]], 'power': [['pizza', 0]], 'functional': [['pizza', 2]], 'pieces': [['pizza', 0]], 'wet': [['pizza', 0]], 'open': [['pizza', 0]], 'temperature': [['pizza', 0]], 'solid': [['pizza', 0]], 'contain': [['pizza', 0]], 'running': [['pizza', 0]], 'moveable': [['pizza', 2]], 'mixed': [['pizza', 0]], 'edible': [['pizza', 0]]}\n","    ],\n","}\n","\n","\n","{\n","  story_id: \n","    13,\n","  worker_id: \n","    A32W24TWSWXW,\n","  type: \n","    order,\n","  idx: \n","    2,\n","  aug: \n","    False,\n","  actor: \n","    John,\n","  location: \n","    kitchen,\n","  objects: \n","    cabinet, counter, knife, pan, potato, pizza,\n","  sentences: \n","    [\n","      John was getting the snacks ready for the party.\n","      John opened the cabinet, took out a pan and put it on the counter.\n","      John put the pizza on the pan and put them into the oven.\n","      John opened the fridge and got out the pizza.\n","      John took a knife and cut the hot pizza in eight slices.\n","    ],\n","  length: \n","    5,\n","  example_id: \n","    13-O2,\n","  plausible: \n","    False,\n","  breakpoint: \n","    3,\n","  confl_sents: \n","    [\n","      2\n","    ],\n","  confl_pairs: \n","    [],\n","  states: \n","    [\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n","      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['knife', 2], ['slices', 2], ['hot pizza', 0]], 'exist': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'clean': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'power': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'functional': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'pieces': [['knife', 0], ['slices', 0], ['hot pizza', 4]], 'wet': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'open': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'temperature': [['knife', 0], ['slices', 0], ['hot pizza', 2]], 'solid': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'contain': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'running': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'moveable': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'mixed': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'edible': [['knife', 0], ['slices', 0], ['hot pizza', 0]]}\n","    ],\n","}\n","\n","\n","{\n","  story_id: \n","    33,\n","  worker_id: \n","    A1F01FVEPYCPHO,\n","  type: \n","    None,\n","  idx: \n","    None,\n","  aug: \n","    False,\n","  actor: \n","    Mary,\n","  location: \n","    bathroom,\n","  objects: \n","    washing machine, cabinet, toothpaste, bleach, socks, mirror,\n","  sentences: \n","    [\n","      Mary took off her socks.\n","      Mary put the socks in the washing machine.\n","      Mary opened the cabinet.\n","      Mary took out the toothbrush and toothpaste.\n","      Mary brushed her teeth while looking in the mirror.\n","    ],\n","  length: \n","    5,\n","  example_id: \n","    33,\n","  plausible: \n","    True,\n","  breakpoint: \n","    -1,\n","  confl_sents: \n","    [],\n","  confl_pairs: \n","    [],\n","  states: \n","    [\n","      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 8]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['socks', 5]], 'exist': [['socks', 2]], 'clean': [['socks', 0]], 'power': [['socks', 0]], 'functional': [['socks', 2]], 'pieces': [['socks', 0]], 'wet': [['socks', 0]], 'open': [['socks', 0]], 'temperature': [['socks', 0]], 'solid': [['socks', 0]], 'contain': [['socks', 0]], 'running': [['socks', 0]], 'moveable': [['socks', 2]], 'mixed': [['socks', 0]], 'edible': [['socks', 0]]}\n","      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['socks', 6], ['washing machine', 0]], 'exist': [['socks', 2], ['washing machine', 2]], 'clean': [['socks', 0], ['washing machine', 0]], 'power': [['socks', 0], ['washing machine', 0]], 'functional': [['socks', 2], ['washing machine', 2]], 'pieces': [['socks', 0], ['washing machine', 0]], 'wet': [['socks', 0], ['washing machine', 0]], 'open': [['socks', 0], ['washing machine', 8]], 'temperature': [['socks', 0], ['washing machine', 0]], 'solid': [['socks', 0], ['washing machine', 0]], 'contain': [['socks', 0], ['washing machine', 6]], 'running': [['socks', 0], ['washing machine', 0]], 'moveable': [['socks', 2], ['washing machine', 2]], 'mixed': [['socks', 0], ['washing machine', 0]], 'edible': [['socks', 0], ['washing machine', 0]]}\n","      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['cabinet', 0]], 'exist': [['cabinet', 2]], 'clean': [['cabinet', 0]], 'power': [['cabinet', 0]], 'functional': [['cabinet', 2]], 'pieces': [['cabinet', 0]], 'wet': [['cabinet', 0]], 'open': [['cabinet', 4]], 'temperature': [['cabinet', 0]], 'solid': [['cabinet', 0]], 'contain': [['cabinet', 0]], 'running': [['cabinet', 0]], 'moveable': [['cabinet', 2]], 'mixed': [['cabinet', 0]], 'edible': [['cabinet', 0]]}\n","      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['toothbrush', 7], ['toothpaste', 7]], 'exist': [['toothbrush', 2], ['toothpaste', 2]], 'clean': [['toothbrush', 0], ['toothpaste', 0]], 'power': [['toothbrush', 0], ['toothpaste', 0]], 'functional': [['toothbrush', 2], ['toothpaste', 2]], 'pieces': [['toothbrush', 0], ['toothpaste', 0]], 'wet': [['toothbrush', 0], ['toothpaste', 0]], 'open': [['toothbrush', 0], ['toothpaste', 0]], 'temperature': [['toothbrush', 0], ['toothpaste', 0]], 'solid': [['toothbrush', 0], ['toothpaste', 0]], 'contain': [['toothbrush', 0], ['toothpaste', 0]], 'running': [['toothbrush', 0], ['toothpaste', 0]], 'moveable': [['toothbrush', 2], ['toothpaste', 2]], 'mixed': [['toothbrush', 0], ['toothpaste', 0]], 'edible': [['toothbrush', 0], ['toothpaste', 0]]}\n","      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 6]], 'location': [['mirror', 0], ['teeth', 0]], 'exist': [['mirror', 2], ['teeth', 2]], 'clean': [['mirror', 0], ['teeth', 6]], 'power': [['mirror', 0], ['teeth', 0]], 'functional': [['mirror', 2], ['teeth', 2]], 'pieces': [['mirror', 0], ['teeth', 0]], 'wet': [['mirror', 0], ['teeth', 0]], 'open': [['mirror', 0], ['teeth', 0]], 'temperature': [['mirror', 0], ['teeth', 0]], 'solid': [['mirror', 0], ['teeth', 0]], 'contain': [['mirror', 0], ['teeth', 0]], 'running': [['mirror', 0], ['teeth', 0]], 'moveable': [['mirror', 2], ['teeth', 0]], 'mixed': [['mirror', 0], ['teeth', 0]], 'edible': [['mirror', 0], ['teeth', 0]]}\n","    ],\n","}\n","\n","\n"]}],"source":["from www.utils import print_dict\n","\n","partitions = ['train', 'dev', 'test']\n","subtasks = ['cloze', 'order']\n","\n","# We can split the data into multiple json files later\n","data_file = os.path.join(DRIVE_PATH, 'all_data/www.json')\n","with open(data_file, 'r') as f:\n","  dataset = json.load(f)\n","\n","print('Preprocessed examples:')\n","for ex_idx in [0,1,5,10]:\n","  ex = dataset['dev'][list(dataset['dev'].keys())[ex_idx]]\n","  print_dict(ex)"]},{"cell_type":"markdown","metadata":{"id":"k_0WsycpFMdb"},"source":["### Data Filtering and Sampling\n","Since there is a big imbalance between plausible/implausible class labels, we will upsample the plausible stories.\n","\n","For now, we will also break the dataset into two sub-datasets: cloze and ordering.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1700954691563,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"-twYzY5rF1Mi"},"outputs":[],"source":["cloze_dataset = {p: [] for p in dataset}\n","order_dataset = {p: [] for p in dataset}\n","\n","for p in dataset:\n","  for exid in dataset[p]:\n","    ex = dataset[p][exid]\n","\n","    if ex['type'] == None:\n","      continue\n","\n","    ex_plaus = dataset[p][str(ex['story_id'])]\n","\n","    if ex['type'] == 'cloze':\n","      cloze_dataset[p].append(ex)\n","      cloze_dataset[p].append(ex_plaus) # For every implausible story, add a copy of its corresponding plausible story\n","\n","    # Exclude augmented ordering examples from dev and test, since the breakpoints aren't always accurate in those\n","    elif ex['type'] == 'order' and not (p != 'train' and ex['aug']):\n","      order_dataset[p].append(ex)\n","      order_dataset[p].append(ex_plaus)"]},{"cell_type":"markdown","metadata":{"id":"Cz5tcmScJrka"},"source":["\n","\n","### Convert TRIP to Two-Story Classification Task\n","\n","Ready the TRIP dataset for two-story classification."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3830,"status":"ok","timestamp":1700954695362,"user":{"displayName":"Junzhu Lu","userId":"04107452450511706345"},"user_tz":300},"id":"Af976ygKJv7W","outputId":"717e7e6e-bd74-442c-9548-315cf4dcf4e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloze label distribution (train):\n","[(1, 400), (0, 399)]\n","Cloze label distribution (dev):\n","[(0, 161), (1, 161)]\n","Cloze label distribution (test):\n","[(1, 176), (0, 175)]\n","{\n","  example_id: \n","    0-C0,\n","  stories: \n","    [\n","      {'story_id': 0, 'worker_id': 'A1F01FVEPYCPHO', 'type': 'cloze', 'idx': 0, 'aug': False, 'actor': 'Tom', 'location': 'kitchen', 'objects': 'dustbin, microwave, pan, plate, cereal, soup', 'sentences': ['Tom bought a new dustbin for the kitchen.', 'Tom threw a broken plate in the dustbin.', 'Tom got some soup from the fridge.', 'Tom put the soup in the microwave.', 'Tom ate the cold soup.'], 'length': 5, 'example_id': '0-C0', 'plausible': False, 'breakpoint': 4, 'confl_sents': [3], 'confl_pairs': [[3, 4]], 'states': [{'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 6]], 'exist': [['dustbin', 4]], 'clean': [['dustbin', 0]], 'power': [['dustbin', 0]], 'functional': [['dustbin', 2]], 'pieces': [['dustbin', 0]], 'wet': [['dustbin', 0]], 'open': [['dustbin', 0]], 'temperature': [['dustbin', 0]], 'solid': [['dustbin', 0]], 'contain': [['dustbin', 0]], 'running': [['dustbin', 0]], 'moveable': [['dustbin', 2]], 'mixed': [['dustbin', 0]], 'edible': [['dustbin', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 0], ['plate', 6]], 'exist': [['dustbin', 2], ['plate', 2]], 'clean': [['dustbin', 0], ['plate', 5]], 'power': [['dustbin', 0], ['plate', 0]], 'functional': [['dustbin', 2], ['plate', 1]], 'pieces': [['dustbin', 0], ['plate', 0]], 'wet': [['dustbin', 0], ['plate', 0]], 'open': [['dustbin', 0], ['plate', 0]], 'temperature': [['dustbin', 0], ['plate', 0]], 'solid': [['dustbin', 0], ['plate', 0]], 'contain': [['dustbin', 6], ['plate', 0]], 'running': [['dustbin', 0], ['plate', 0]], 'moveable': [['dustbin', 0], ['plate', 2]], 'mixed': [['dustbin', 0], ['plate', 0]], 'edible': [['dustbin', 0], ['plate', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['fridge', 0], ['soup', 2]], 'exist': [['fridge', 2], ['soup', 2]], 'clean': [['fridge', 0], ['soup', 0]], 'power': [['fridge', 0], ['soup', 0]], 'functional': [['fridge', 2], ['soup', 2]], 'pieces': [['fridge', 0], ['soup', 0]], 'wet': [['fridge', 0], ['soup', 0]], 'open': [['fridge', 8], ['soup', 0]], 'temperature': [['fridge', 0], ['soup', 1]], 'solid': [['fridge', 0], ['soup', 0]], 'contain': [['fridge', 8], ['soup', 0]], 'running': [['fridge', 0], ['soup', 0]], 'moveable': [['fridge', 2], ['soup', 2]], 'mixed': [['fridge', 0], ['soup', 0]], 'edible': [['fridge', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0], ['soup', 3]], 'exist': [['microwave', 2], ['soup', 2]], 'clean': [['microwave', 0], ['soup', 0]], 'power': [['microwave', 2], ['soup', 0]], 'functional': [['microwave', 2], ['soup', 2]], 'pieces': [['microwave', 0], ['soup', 0]], 'wet': [['microwave', 0], ['soup', 0]], 'open': [['microwave', 8], ['soup', 0]], 'temperature': [['microwave', 0], ['soup', 0]], 'solid': [['microwave', 0], ['soup', 0]], 'contain': [['microwave', 6], ['soup', 0]], 'running': [['microwave', 0], ['soup', 0]], 'moveable': [['microwave', 2], ['soup', 2]], 'mixed': [['microwave', 0], ['soup', 0]], 'edible': [['microwave', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['soup', 1]], 'exist': [['soup', 3]], 'clean': [['soup', 0]], 'power': [['soup', 0]], 'functional': [['soup', 2]], 'pieces': [['soup', 0]], 'wet': [['soup', 0]], 'open': [['soup', 0]], 'temperature': [['soup', 7]], 'solid': [['soup', 0]], 'contain': [['soup', 0]], 'running': [['soup', 0]], 'moveable': [['soup', 2]], 'mixed': [['soup', 0]], 'edible': [['soup', 0]]}]}\n","      {'story_id': 0, 'worker_id': 'A1F01FVEPYCPHO', 'type': None, 'idx': None, 'aug': False, 'actor': 'Tom', 'location': 'kitchen', 'objects': 'dustbin, microwave, pan, plate, cereal, soup', 'sentences': ['Tom bought a new dustbin for the kitchen.', 'Tom threw a broken plate in the dustbin.', 'Tom got some soup from the fridge.', 'Tom put the soup in the microwave.', 'Tom turned on the microwave.'], 'length': 5, 'example_id': '0', 'plausible': True, 'breakpoint': -1, 'confl_sents': [], 'confl_pairs': [], 'states': [{'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 6]], 'exist': [['dustbin', 4]], 'clean': [['dustbin', 0]], 'power': [['dustbin', 0]], 'functional': [['dustbin', 2]], 'pieces': [['dustbin', 0]], 'wet': [['dustbin', 0]], 'open': [['dustbin', 0]], 'temperature': [['dustbin', 0]], 'solid': [['dustbin', 0]], 'contain': [['dustbin', 0]], 'running': [['dustbin', 0]], 'moveable': [['dustbin', 2]], 'mixed': [['dustbin', 0]], 'edible': [['dustbin', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 0], ['plate', 6]], 'exist': [['dustbin', 2], ['plate', 2]], 'clean': [['dustbin', 0], ['plate', 5]], 'power': [['dustbin', 0], ['plate', 0]], 'functional': [['dustbin', 2], ['plate', 1]], 'pieces': [['dustbin', 0], ['plate', 0]], 'wet': [['dustbin', 0], ['plate', 0]], 'open': [['dustbin', 0], ['plate', 0]], 'temperature': [['dustbin', 0], ['plate', 0]], 'solid': [['dustbin', 0], ['plate', 0]], 'contain': [['dustbin', 6], ['plate', 0]], 'running': [['dustbin', 0], ['plate', 0]], 'moveable': [['dustbin', 0], ['plate', 2]], 'mixed': [['dustbin', 0], ['plate', 0]], 'edible': [['dustbin', 0], ['plate', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['fridge', 0], ['soup', 2]], 'exist': [['fridge', 2], ['soup', 2]], 'clean': [['fridge', 0], ['soup', 0]], 'power': [['fridge', 0], ['soup', 0]], 'functional': [['fridge', 2], ['soup', 2]], 'pieces': [['fridge', 0], ['soup', 0]], 'wet': [['fridge', 0], ['soup', 0]], 'open': [['fridge', 8], ['soup', 0]], 'temperature': [['fridge', 0], ['soup', 1]], 'solid': [['fridge', 0], ['soup', 0]], 'contain': [['fridge', 8], ['soup', 0]], 'running': [['fridge', 0], ['soup', 0]], 'moveable': [['fridge', 2], ['soup', 2]], 'mixed': [['fridge', 0], ['soup', 0]], 'edible': [['fridge', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0], ['soup', 3]], 'exist': [['microwave', 2], ['soup', 2]], 'clean': [['microwave', 0], ['soup', 0]], 'power': [['microwave', 2], ['soup', 0]], 'functional': [['microwave', 2], ['soup', 2]], 'pieces': [['microwave', 0], ['soup', 0]], 'wet': [['microwave', 0], ['soup', 0]], 'open': [['microwave', 8], ['soup', 0]], 'temperature': [['microwave', 0], ['soup', 0]], 'solid': [['microwave', 0], ['soup', 0]], 'contain': [['microwave', 6], ['soup', 0]], 'running': [['microwave', 0], ['soup', 0]], 'moveable': [['microwave', 2], ['soup', 2]], 'mixed': [['microwave', 0], ['soup', 0]], 'edible': [['microwave', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0]], 'exist': [['microwave', 2]], 'clean': [['microwave', 0]], 'power': [['microwave', 2]], 'functional': [['microwave', 2]], 'pieces': [['microwave', 0]], 'wet': [['microwave', 0]], 'open': [['microwave', 1]], 'temperature': [['microwave', 0]], 'solid': [['microwave', 0]], 'contain': [['microwave', 2]], 'running': [['microwave', 4]], 'moveable': [['microwave', 2]], 'mixed': [['microwave', 0]], 'edible': [['microwave', 0]]}]}\n","    ],\n","  length: \n","    5,\n","  label: \n","    1,\n","  breakpoint: \n","    4,\n","  confl_sents: \n","    [\n","      3\n","    ],\n","  confl_pairs: \n","    [\n","      [3, 4]\n","    ],\n","}\n","\n","\n"]}],"source":["from www.utils import print_dict\n","import json\n","from collections import Counter\n","\n","data_file = os.path.join(DRIVE_PATH, 'all_data/www_2s_new.json')\n","with open(data_file, 'r') as f:\n","  cloze_dataset_2s, order_dataset_2s = json.load(f)\n","\n","for p in cloze_dataset_2s:\n","  label_dist = Counter([ex['label'] for ex in cloze_dataset_2s[p]])\n","  print('Cloze label distribution (%s):' % p)\n","  print(label_dist.most_common())\n","print_dict(cloze_dataset_2s['train'][0])"]},{"cell_type":"markdown","metadata":{"id":"BxIYaEobhR7J"},"source":["---\n","\n","# TRIP Results\n","\n","Contains code for the tiered and random TRIP baselines."]},{"cell_type":"markdown","metadata":{"id":"v7VlN2jUwvcC"},"source":["## Random Tiered Classifier for TRIP\n","\n","For the random baseline, we average the results of 10 runs. Running the below will report (mean, variance) for each evaluation partition."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"OGYE2UIiASDv"},"outputs":[{"name":"stderr","output_type":"stream","text":["[========================================================================] 100%\n","[========================================================================] 100%\n","[========================================================================] 100%\n"]}],"source":["from www.dataset.prepro import get_tiered_data\n","from www.dataset.featurize import add_bert_features_tiered, get_tensor_dataset_tiered\n","from collections import Counter\n","import numpy as np\n","\n","from www.dataset.ann import att_to_num_classes, idx_to_att\n","from sklearn.metrics import accuracy_score, f1_score\n","from www.utils import print_dict\n","\n","tiered_dataset = cloze_dataset_2s\n","\n","seq_length = 16 # Max sequence length to pad to\n","\n","tiered_dataset = get_tiered_data(tiered_dataset)\n","tiered_dataset = add_bert_features_tiered(tiered_dataset, tokenizer, seq_length, add_segment_ids=True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# 595 final project configurations\n","freeze = False"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wL20bloxwxci"},"outputs":[{"name":"stdout","output_type":"stream","text":["starting dev...\n","starting run 0...\n","starting run 1...\n","starting run 2...\n","starting run 3...\n","starting run 4...\n","starting run 5...\n","starting run 6...\n","starting run 7...\n","starting run 8...\n","starting run 9...\n","RANDOM BASELINE (dev, 10 runs)\n","{\n","  story_accuracy: \n","    (0.49505582667849746, 0.008213817419306007),\n","  confl_f1: \n","    (0.48453221816295977, 0.0013040154050816186),\n","  precondition_f1: \n","    (0.040297812444650326, 8.961890559634094e-05),\n","  effect_f1: \n","    (0.04032812881069694, 9.472913525806349e-05),\n","  verifiability: \n","    (0.0, 0.0),\n","  consistency: \n","    (0.12041469486345262, 0.003250111885221874),\n","}\n","\n","\n","starting test...\n","starting run 0...\n","starting run 1...\n","starting run 2...\n","starting run 3...\n","starting run 4...\n","starting run 5...\n","starting run 6...\n","starting run 7...\n","starting run 8...\n","starting run 9...\n","RANDOM BASELINE (test, 10 runs)\n","{\n","  story_accuracy: \n","    (0.49836237928020255, 0.0038938303319187256),\n","  confl_f1: \n","    (0.4836060948738846, 0.00025603565579520557),\n","  precondition_f1: \n","    (0.04001988622427764, 6.365665931327992e-05),\n","  effect_f1: \n","    (0.04029473361089536, 2.6119362041871277e-05),\n","  verifiability: \n","    (0.0, 0.0),\n","  consistency: \n","    (0.10662819929422918, 0.003475007425085269),\n","}\n","\n","\n"]}],"source":["from www.dataset.prepro import get_tiered_data, balance_labels\n","from www.dataset.featurize import add_bert_features_tiered, get_tensor_dataset_tiered\n","from collections import Counter\n","import numpy as np\n","from www.dataset.ann import att_to_num_classes, idx_to_att, att_default_values\n","from sklearn.metrics import accuracy_score, f1_score\n","from www.utils import print_dict\n","import numpy as np\n","\n","# Have to add BERT input IDs and tensorize again\n","num_runs = 10\n","stories = []\n","pred_stories = []\n","conflicts = []\n","pred_conflicts = []\n","preconditions = []\n","pred_preconditions = []\n","effects = []\n","pred_effects = []\n","verifiability = []\n","consistency = []\n","for p in tiered_dataset:\n","  if p == 'train':\n","    continue\n","  metr_avg = {}\n","  print('starting %s...' % p)\n","  for r in range(num_runs):\n","    print('starting run %s...' % str(r))\n","    for ex in tiered_dataset[p]:\n","      verifiable = True\n","      consistent = True\n","\n","      stories.append(ex['label'])\n","      pred_stories.append(np.random.randint(2))\n","\n","      if stories[-1] != pred_stories[-1]:\n","        verifiable = False\n","\n","      labels_ex_p = []\n","      preds_ex_p = []\n","\n","      labels_ex_e = []\n","      preds_ex_e = []\n","\n","      labels_ex_c = []\n","      preds_ex_c = []\n","\n","      for si, story in enumerate(ex['stories']):\n","        labels_story_p = []\n","        preds_story_p = []\n","\n","        labels_story_e = []\n","        preds_story_e = []\n","\n","        for ent_ann in story['entities']:\n","          entity = ent_ann['entity']\n","\n","          if si == 1 - ex['label']:\n","            labels_ex_c.append(ent_ann['conflict_span_onehot'])\n","            pred = np.zeros(ent_ann['conflict_span_onehot'].shape)\n","            for cs in np.random.choice(len(pred), size=2, replace=False):\n","              pred[cs] = 1\n","            preds_ex_c.append(pred)\n","\n","          labels_ent = []\n","          preds_ent = []\n","          for s, sent_ann in enumerate(ent_ann['preconditions']):\n","            if s < len(story['sentences']):\n","              if entity in story['sentences'][s]:\n","\n","                labels_ent.append(sent_ann)\n","                sent_ann_pred = []\n","                for i, l in enumerate(sent_ann):\n","                  pl = np.random.randint(att_to_num_classes[idx_to_att[i]])\n","                  if pl > 0 and pl != att_default_values[idx_to_att[i]]:\n","                    if pl != l:\n","                      verifiable = False\n","                  sent_ann_pred.append(pl)\n","                preds_ent.append(sent_ann_pred)\n","\n","          labels_story_p.append(labels_ent)\n","          preds_story_p.append(preds_ent)\n","\n","          labels_ent = []\n","          preds_ent = []\n","          for s, sent_ann in enumerate(ent_ann['effects']):\n","            if s < len(story['sentences']):\n","              if entity in story['sentences'][s]:\n","\n","                labels_ent.append(sent_ann)\n","                sent_ann_pred = []\n","                for i, l in enumerate(sent_ann):\n","                  pl = np.random.randint(att_to_num_classes[idx_to_att[i]])\n","                  if pl > 0 and pl != att_default_values[idx_to_att[i]]:\n","                    if pl != l:\n","                      verifiable = False\n","                  sent_ann_pred.append(pl)\n","                preds_ent.append(sent_ann_pred)\n","\n","          labels_story_e.append(labels_ent)\n","          preds_story_e.append(preds_ent)\n","\n","        labels_ex_p.append(labels_story_p)\n","        preds_ex_p.append(preds_story_p)\n","\n","        labels_ex_e.append(labels_story_e)\n","        preds_ex_e.append(preds_story_e)\n","\n","      conflicts.append(labels_ex_c)\n","      pred_conflicts.append(preds_ex_c)\n","\n","      preconditions.append(labels_ex_p)\n","      pred_preconditions.append(preds_ex_p)\n","\n","      effects.append(labels_ex_e)\n","      pred_effects.append(preds_ex_e)\n","\n","      p_confl = np.nonzero(np.sum(np.array(preds_ex_c), axis=0))[0]\n","      l_confl = np.nonzero(np.sum(np.array(labels_ex_c), axis=0))[0]\n","      assert len(l_confl) == 2, str(labels_ex_c)\n","      if not (p_confl[0] == l_confl[0] and p_confl[1] == l_confl[1]):\n","        verifiable = False\n","        consistent = False\n","\n","      verifiability.append(1 if verifiable else 0)\n","      consistency.append(1 if consistent else 0)\n","\n","    # Compute metrics\n","    metr = {}\n","    metr['story_accuracy'] = accuracy_score(stories, pred_stories)\n","\n","    conflicts_flat = [c for c_ex in conflicts for c_ent in c_ex for c in c_ent]\n","    pred_conflicts_flat = [c for c_ex in pred_conflicts for c_ent in c_ex for c in c_ent]\n","    metr['confl_f1'] = f1_score(conflicts_flat, pred_conflicts_flat, average='macro')\n","\n","    preconditions_flat = [p for p_ex in preconditions for p_story in p_ex for p_sent in p_story for p_ent in p_sent for p in p_ent]\n","    pred_preconditions_flat = [p for p_ex in pred_preconditions for p_story in p_ex for p_sent in p_story for p_ent in p_sent for p in p_ent]\n","    metr['precondition_f1'] = f1_score(preconditions_flat, pred_preconditions_flat, average='macro')\n","\n","    effects_flat = [p for p_ex in effects for p_story in p_ex for p_sent in p_story for p_ent in p_sent for p in p_ent]\n","    pred_effects_flat = [p for p_ex in pred_effects for p_story in p_ex for p_sent in p_story for p_ent in p_sent for p in p_ent]\n","    metr['effect_f1'] = f1_score(effects_flat, pred_effects_flat, average='macro')\n","\n","    metr['verifiability'] = np.mean(verifiability)\n","    metr['consistency'] = np.mean(consistency)\n","\n","    for k in metr:\n","      if k not in metr_avg:\n","        metr_avg[k] = []\n","      metr_avg[k].append(metr[k])\n","\n","  for k in metr_avg:\n","    metr_avg[k] = (np.mean(metr_avg[k]), np.var(metr_avg[k]) ** 0.5)\n","  print('RANDOM BASELINE (%s, %s runs)' % (str(p), str(num_runs)))\n","  print_dict(metr_avg)"]},{"cell_type":"markdown","metadata":{"id":"5ctQweSlAceo"},"source":["## Transformer-Based Tiered Classifier for TRIP\n","\n","This is the baseline model presented in the paper. Based on the settings above, the below cells can be used for training and evaluating models.\n"]},{"cell_type":"markdown","metadata":{"id":"0q-xjfYU_cV8"},"source":["### Featurization for Tiered Classification\n","\n","Get the data ready for input to the model."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"gLCJXeMb_cV9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[========================================================================] 100%\n","[========================================================================] 100%\n","[========================================================================] 100%\n","/home/nliang/595project/www/dataset/featurize.py:52: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n","  all_input_ids = torch.tensor([[[[story['entities'][e]['input_ids'][s] if e < len(story['entities']) else np.zeros((seq_length)) for s in range(max_sentences)] for e in range(max_entities)] for story in ex_2s['stories']] for ex_2s in dataset])\n"]}],"source":["from www.dataset.prepro import get_tiered_data, balance_labels\n","from www.dataset.featurize import add_bert_features_tiered, get_tensor_dataset_tiered\n","from collections import Counter\n","\n","tiered_dataset = cloze_dataset_2s\n","\n","# Debug the code on a small amount of data\n","if debug:\n","  for k in tiered_dataset:\n","    tiered_dataset[k] = tiered_dataset[k][:20]\n","\n","# train_spans = True\n","train_spans = False\n","if train_spans:\n","  tiered_dataset = get_story_spans_2s(tiered_dataset, train_only=True)\n","  tiered_dataset['train'] = [ex for ex in tiered_dataset['train'] if ex['label'] != -1] # For now, ignore examples where both stories are plausible :(\n","\n","seq_length = 16 # Max sequence length to pad to\n","\n","tiered_dataset = get_tiered_data(tiered_dataset)\n","tiered_dataset = add_bert_features_tiered(tiered_dataset, tokenizer, seq_length, add_segment_ids=True)\n","\n","tiered_tensor_dataset = {}\n","max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n","for p in tiered_dataset:\n","  tiered_tensor_dataset[p] = get_tensor_dataset_tiered(tiered_dataset[p], max_story_length, add_segment_ids=True)"]},{"cell_type":"markdown","metadata":{"id":"-fQ6wXQIBdq1"},"source":["### Train Models"]},{"cell_type":"markdown","metadata":{"id":"U-BMInyrBdq2"},"source":["#### Configure Hyperparameters\n","We will perform grid search over (batch size, learning rate). Configure the training sub-task, search space and set the maximum number of training epochs here. Currently configured for re-training the best RoBERTa-based model instance. Read code comments for more information.\n","\n","**Additional configuration options:**\n","* Change the `generate_learning_curve` variable to `True` to generate data for training curves in the style presented in the paper.\n","* You may ablate the input to the Conflict Detector based on a few pre-defined ablation modes. To do so, change the `ablation` variable based on the comments in the code."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"tvfTuEYRBdq3"},"outputs":[],"source":["from www.dataset.ann import att_to_idx, att_to_num_classes, att_types\n","\n","subtask = 'cloze'\n","batch_sizes = [config_batch_size]\n","learning_rates = [config_lr]\n","epochs = config_epochs\n","eval_batch_size = 16\n","generate_learning_curve = True # Generate data for training curve figure in TRIP paper\n","\n","num_state_labels = {}\n","for att in att_to_idx:\n","  if att_types[att] == 'default':\n","    num_state_labels[att_to_idx[att]] = 3\n","  else:\n","    num_state_labels[att_to_idx[att]] = att_to_num_classes[att] # Location attributes fall into this since they don't have well-define pre- and post-condition yet\n","\n","# Ablation options:\n","# - attributes: skip attribute prediction phase\n","# - embeddings: DON'T input contextual embeddings to conflict detector\n","# - states: DON'T input states to conflict detector\n","# - states-labels: in states input to conflict detector, include predicted labels\n","# - states-logits: in states input to conflict detector, include state logits (preferred)\n","# - states-teacher-forcing: train conflict detector on ground truth state labels (not predictions)\n","# - states-attention: re-weight input to conflict detector with weights conditioned on states representation\n","#ablation = ['attributes', 'states-logits', 'states-attention'] \n","ablation = ['attributes', 'states-logits'] # This is the default mode presented in the paper"]},{"cell_type":"markdown","metadata":{"id":"8fRC3cnLBdq3"},"source":["#### Perform Grid Search\n","\n","Perform hyperparameter tuning to find the best story classification model.\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# 595 final project configurations\n","freeze = False\n","dynamic_weight = False\n","cross_attn = True"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#!nvidia-smi\n","#!ps -u -p 431104 \n","#!ps -u -p 432143\n","#!kill -9 431104\n","#!kill -9 432143"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"DWnCen7NBdq3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Beginning grid search for the cloze sub-task over 1 parameter combination(s)!\n","\n","TRAINING MODEL: bs=1, lr=1e-05\n"]},{"name":"stderr","output_type":"stream","text":["/home/nliang/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[                                                                        ]   0%\r"]},{"name":"stdout","output_type":"stream","text":["[0] Beginning epoch...\n","(12236, 7) (322, 2, 19, 7)\n"]},{"name":"stderr","output_type":"stream","text":["[##############                                                          ]  20%\r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/nliang/595project/Verifiable-Coherent-NLU.ipynb Cell 48\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m   \u001b[39m# Train the model for one epoch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m] Beginning epoch...\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(epoch))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m   epoch_loss, _ \u001b[39m=\u001b[39m train_epoch_tiered(model, freeze, dynamic_weight, optimizer, train_dataloader, device, epoch, seg_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m                                      build_learning_curves\u001b[39m=\u001b[39;49mgenerate_learning_curve, val_dataloader\u001b[39m=\u001b[39;49mdev_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m                                      train_lc_data\u001b[39m=\u001b[39;49mtrain_lc_data, val_lc_data\u001b[39m=\u001b[39;49mval_lc_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m   \u001b[39m# Save loss\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmbit10.eecs.umich.edu/home/nliang/595project/Verifiable-Coherent-NLU.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m   loss_values\u001b[39m.\u001b[39mappend(epoch_loss)\n","File \u001b[0;32m~/595project/www/model/train.py:241\u001b[0m, in \u001b[0;36mtrain_epoch_tiered\u001b[0;34m(model, freeze, dynamic_weight, optimizer, train_dataloader, device, epoch, seg_mode, return_losses, build_learning_curves, val_dataloader, train_lc_data, val_lc_data)\u001b[0m\n\u001b[1;32m    239\u001b[0m chunk_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m-\u001b[39m step \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m chunk_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m   validation_results \u001b[39m=\u001b[39m evaluate_tiered(model, val_dataloader, device, [(accuracy_score, \u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m), (f1_score, \u001b[39m'\u001b[39;49m\u001b[39mf1\u001b[39;49m\u001b[39m'\u001b[39;49m)], seg_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_explanations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_losses\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    242\u001b[0m   out \u001b[39m=\u001b[39m validation_results[\u001b[39m16\u001b[39m]\n\u001b[1;32m    244\u001b[0m   val_record \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(val_lc_data) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    245\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39miteration\u001b[39m\u001b[39m'\u001b[39m: (\u001b[39mlen\u001b[39m(val_lc_data) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m+\u001b[39m step,\n\u001b[1;32m    246\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mloss_preconditions\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mfloat\u001b[39m(out[\u001b[39m'\u001b[39m\u001b[39mloss_preconditions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()) \u001b[39m/\u001b[39m model\u001b[39m.\u001b[39mnum_attributes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mloss_stories\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mfloat\u001b[39m(out[\u001b[39m'\u001b[39m\u001b[39mloss_stories\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()),\n\u001b[1;32m    250\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mloss_total\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mfloat\u001b[39m(out[\u001b[39m'\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())}\n","File \u001b[0;32m~/595project/www/model/eval.py:288\u001b[0m, in \u001b[0;36mevaluate_tiered\u001b[0;34m(model, eval_dataloader, device, metrics, seg_mode, return_softmax, return_explanations, return_losses, verbose)\u001b[0m\n\u001b[1;32m    280\u001b[0m batch_size, num_stories, num_entities, num_sents, seq_length \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape\n\u001b[1;32m    282\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    283\u001b[0m   \u001b[39m# out = model(input_ids,\u001b[39;00m\n\u001b[1;32m    284\u001b[0m   \u001b[39m#             input_lengths,\u001b[39;00m\n\u001b[1;32m    285\u001b[0m   \u001b[39m#             input_entities,\u001b[39;00m\n\u001b[1;32m    286\u001b[0m   \u001b[39m#             attention_mask=input_mask,\u001b[39;00m\n\u001b[1;32m    287\u001b[0m   \u001b[39m#             token_type_ids=segment_ids)\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m   out \u001b[39m=\u001b[39m model(input_ids,\n\u001b[1;32m    289\u001b[0m               input_lengths,\n\u001b[1;32m    290\u001b[0m               input_entities,\n\u001b[1;32m    291\u001b[0m               attention_mask\u001b[39m=\u001b[39;49minput_mask,\n\u001b[1;32m    292\u001b[0m               token_type_ids\u001b[39m=\u001b[39;49msegment_ids,\n\u001b[1;32m    293\u001b[0m               attributes\u001b[39m=\u001b[39;49mattributes,\n\u001b[1;32m    294\u001b[0m               preconditions\u001b[39m=\u001b[39;49mpreconditions,\n\u001b[1;32m    295\u001b[0m               effects\u001b[39m=\u001b[39;49meffects,\n\u001b[1;32m    296\u001b[0m               conflicts\u001b[39m=\u001b[39;49mconflicts,\n\u001b[1;32m    297\u001b[0m               labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m return_losses:\n\u001b[1;32m    299\u001b[0m   \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m out:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/595project/www/model/transformers_ext.py:327\u001b[0m, in \u001b[0;36mTieredModelPipeline.forward\u001b[0;34m(self, input_ids, input_lengths, input_entities, attention_mask, token_type_ids, attributes, preconditions, effects, conflicts, labels, training)\u001b[0m\n\u001b[1;32m    325\u001b[0m   loss_preconditions \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    326\u001b[0m precond_for_effect \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 327\u001b[0m out_preconditions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros((batch_size \u001b[39m*\u001b[39;49m num_stories \u001b[39m*\u001b[39;49m num_entities \u001b[39m*\u001b[39;49m num_sents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_attributes))\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    328\u001b[0m out_preconditions_softmax \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    329\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attributes):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","from www.model.train import train_epoch_tiered\n","import importlib\n","import www.model.eval\n","import www.model.transformers_ext\n","importlib.reload(www.model.eval)\n","importlib.reload(www.model.transformers_ext)\n","import www.model.eval\n","from www.model.eval import evaluate_tiered, save_results, save_preds, add_entity_attribute_labels\n","from sklearn.metrics import accuracy_score, f1_score\n","from www.utils import print_dict, get_model_dir\n","from www.model.transformers_ext import TieredModelPipeline\n","from www.dataset.ann import att_to_num_classes\n","import shutil\n","import pandas as pd\n","\n","\n","\n","seed_val = 22 # Save random seed for reproducibility\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll keep the validation data here with a constant eval batch size\n","dev_sampler = SequentialSampler(tiered_tensor_dataset['dev'])\n","dev_dataloader = DataLoader(tiered_tensor_dataset['dev'], sampler=dev_sampler, batch_size=eval_batch_size)\n","dev_dataset_name = subtask + '_%s_dev'\n","dev_ids = [ex['example_id'] for ex in tiered_dataset['dev']]\n","\n","all_losses = []\n","param_combos = []\n","combo_names = []\n","all_val_objs = []\n","output_dirs = []\n","best_obj = 0.0\n","best_model = '<none>'\n","best_dir = ''\n","best_obj2 = 0.0\n","best_model2 = '<none>'\n","best_dir2 = ''\n","\n","print('Beginning grid search for the %s sub-task over %s parameter combination(s)!' % (subtask, str(len(batch_sizes) * len(learning_rates))))\n","for bs in batch_sizes:\n","  for lr in learning_rates:\n","    print('\\nTRAINING MODEL: bs=%s, lr=%s' % (str(bs), str(lr)))\n","\n","    loss_values = []\n","    obj_values = []\n","\n","    # Set up training dataset with new batch size\n","    train_sampler = RandomSampler(tiered_tensor_dataset['train'])\n","    train_dataloader = DataLoader(tiered_tensor_dataset['train'], sampler=train_sampler, batch_size=bs)\n","\n","    # Set up model\n","    config = config_class.from_pretrained(model_name,\n","                                          cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n","    emb = emb_class.from_pretrained(model_name,\n","                                          config=config,\n","                                          cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n","    if torch.cuda.is_available():\n","      emb.cuda()\n","    device = emb.device\n","    max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n","\n","    # if freeze, change the modules\n","    if freeze:\n","      probe_model = \"roberta-large_cloze_1_1e-05_1_0.0-0.3-0.3-0.4-0.0_tiered_pipeline_lc_ablate_attributes_states-logits\"\n","      probe_model = os.path.join(DRIVE_PATH, 'saved_models', probe_model)\n","      probe_model = torch.load(os.path.join(probe_model, 'classifiers.pth'))\n","      model = TieredModelPipeline(probe_model.embedding, max_story_length, len(att_to_num_classes), num_state_labels,\n","                               config_class, model_name, device,\n","                               ablation=ablation, loss_weights=loss_weights).to(device)\n","      model.precondition_classifiers = probe_model.precondition_classifiers\n","      model.effect_classifiers = probe_model.effect_classifiers\n","    else:\n","       model = TieredModelPipeline(emb, max_story_length, len(att_to_num_classes), num_state_labels,\n","                                config_class, model_name, device,\n","                                ablation=ablation,cross_attn=True, loss_weights=loss_weights).to(device)\n","\n","\n","    # Set up optimizer\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    total_steps = len(train_dataloader) * epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n","\n","    train_lc_data = []\n","    val_lc_data = []\n","    epochs = 10\n","    for epoch in range(epochs):\n","      # Train the model for one epoch\n","      print('[%s] Beginning epoch...' % str(epoch))\n","\n","      epoch_loss, _ = train_epoch_tiered(model, freeze, dynamic_weight, optimizer, train_dataloader, device, epoch, seg_mode=False,\n","                                         build_learning_curves=generate_learning_curve, val_dataloader=dev_dataloader,\n","                                         train_lc_data=train_lc_data, val_lc_data=val_lc_data)\n","\n","      # Save loss\n","      loss_values.append(epoch_loss)\n","\n","      # Validate on dev set\n","      validation_results = evaluate_tiered(model, dev_dataloader, device, [(accuracy_score, 'accuracy'), (f1_score, 'f1')], seg_mode=False, return_explanations=True)\n","      metr_attr, all_pred_atts, all_atts, \\\n","      metr_prec, all_pred_prec, all_prec, \\\n","      metr_eff, all_pred_eff, all_eff, \\\n","      metr_conflicts, all_pred_conflicts, all_conflicts, \\\n","      metr_stories, all_pred_stories, all_stories, explanations = validation_results[:16]\n","      explanations = add_entity_attribute_labels(explanations, tiered_dataset['dev'], list(att_to_num_classes.keys()))\n","\n","      print('[%s] Validation results:' % str(epoch))\n","      print('[%s] Preconditions:' % str(epoch))\n","      print_dict(metr_prec)\n","      print('[%s] Effects:' % str(epoch))\n","      print_dict(metr_eff)\n","      print('[%s] Conflicts:' % str(epoch))\n","      print_dict(metr_conflicts)\n","      print('[%s] Stories:' % str(epoch))\n","      print_dict(metr_stories)\n","\n","      # Save accuracy - want to maximize verifiability of tiered predictions\n","      ver = metr_stories['verifiability']\n","      acc = metr_stories['accuracy']\n","      obj_values.append(ver)\n","\n","      # Save model checkpoint\n","      print('[%s] Saving model checkpoint...' % str(epoch))\n","      model_param_str = get_model_dir(model_name.replace('/', '-'), subtask, bs, lr, epoch) + '_' +  '-'.join([str(lw) for lw in loss_weights]) +  '_tiered_pipeline_lc'\n","      if train_spans:\n","        model_param_str += 'spans'\n","      if len(model.ablation) > 0:\n","        model_param_str += '_ablate_'\n","        model_param_str += '_'.join(model.ablation)\n","      output_dir = os.path.join(DRIVE_PATH, 'saved_models', model_param_str)\n","      output_dirs.append(output_dir)\n","      if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","      save_results(metr_attr, output_dir, dev_dataset_name % 'attributes')\n","      save_results(metr_prec, output_dir, dev_dataset_name % 'preconditions')\n","      save_results(metr_eff, output_dir, dev_dataset_name % 'effects')\n","      save_results(metr_conflicts, output_dir, dev_dataset_name % 'conflicts')\n","      save_results(metr_stories, output_dir, dev_dataset_name % 'stories')\n","      save_results(explanations, output_dir, dev_dataset_name % 'explanations')\n","\n","      # Just save story preds\n","      save_preds(dev_ids, all_stories, all_pred_stories, output_dir, dev_dataset_name % 'stories')\n","\n","      emb = emb.module if hasattr(emb, 'module') else emb\n","      emb.save_pretrained(output_dir)\n","      torch.save(model, os.path.join(output_dir, 'classifiers.pth'))\n","      tokenizer.save_vocabulary(output_dir)\n","\n","      if ver > best_obj:\n","        best_obj = ver\n","        best_model = model_param_str\n","        best_dir = output_dir\n","      if acc > best_obj2:\n","        best_obj2 = acc\n","        best_model2 = model_param_str\n","        best_dir2 = output_dir\n","\n","      for od in output_dirs:\n","        if od != best_dir and od != best_dir2 and os.path.exists(od):\n","          shutil.rmtree(od)\n","\n","      print('[%s] Finished epoch.' % str(epoch))\n","\n","    all_losses.append(loss_values)\n","    all_val_objs.append(obj_values)\n","    param_combos.append((bs, lr))\n","    combo_names.append('bs=%s, lr=%s' % (str(bs), str(lr)))\n","\n","print('Finished grid search! :)')\n","print('Best validation *verifiability* %s from model %s.' % (str(best_obj), best_model))\n","print('Best validation *accuracy* %s from model %s.' % (str(best_obj2), best_model2))\n","\n","if generate_learning_curve:\n","  print('Saving learning curve data...')\n","  train_lc_data = [subrecord for record in train_lc_data for subrecord in record] # flatten\n","  val_lc_data = [subrecord for record in val_lc_data for subrecord in record] # flatten\n","\n","  train_lc_data = pd.DataFrame(train_lc_data)\n","  print(os.path.join(best_dir if best_dir != '<none>' else best_dir2, 'learning_curve_data_train.csv'))\n","  train_lc_data.to_csv(os.path.join(best_dir if best_dir != '' else best_dir2, 'learning_curve_data_train.csv'), index=False)\n","  val_lc_data = pd.DataFrame(val_lc_data)\n","  val_lc_data.to_csv(os.path.join(best_dir if best_dir != '' else best_dir2, 'learning_curve_data_val.csv'), index=False)\n","  print('Learning curve data saved. %s rows saved for training, %s rows saved for validation.' % (str(len(train_lc_data.index)), str(len(val_lc_data.index))))"]},{"cell_type":"markdown","metadata":{"id":"aWFmGRhznl2T"},"source":["### Test Models\n","\n","Evaluate accuracy, consistency, and verifiability on the test set."]},{"cell_type":"markdown","metadata":{"id":"Rbvpm9irn3qL"},"source":["#### Load the Trained Model\n","\n","Load the trained model we want to probe and select the appropriate dataset. Paths to the pre-trained models presented in the paper are already provided (download links are found in GitHub repo)."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"fM_lYqw9n3qM"},"outputs":[],"source":["importlib.reload(www.model.transformers_ext)\n","import www.model.transformers_ext\n","from www.model.transformers_ext import TieredModelPipeline\n","from www.dataset.ann import att_to_num_classes, att_to_idx, att_types\n","\n","eval_model_dir = 'best_now_cross_attetion_epoch6'\n","probe_model = eval_model_dir\n","probe_model = os.path.join(DRIVE_PATH, 'saved_models', probe_model)\n","\n","ablation = ['attributes', 'states-logits']\n","\n","if 'cloze' in probe_model:\n","  subtask = 'cloze'\n","elif 'order' in probe_model:\n","  subtask = 'order'\n","\n","if subtask == 'cloze':\n","  subtask_dataset = cloze_dataset_2s\n","elif subtask == 'order':\n","  subtask_dataset = order_dataset_2s\n","\n","# Load the model\n","model = None\n","# model = torch.load(os.path.join(probe_model, 'classifiers.pth'), map_location=torch.device('cpu'))\n","model = torch.load(os.path.join(probe_model, 'classifiers.pth'))\n","if torch.cuda.is_available():\n","  model.cuda()\n","device = model.embedding.device\n","\n","for layer in model.precondition_classifiers:\n","  layer.eval()\n","for layer in model.effect_classifiers:\n","  layer.eval()"]},{"cell_type":"markdown","metadata":{"id":"RfXiCTA9KPjG"},"source":["#### Test the Model\n","\n","Run inference on the testing set of TRIP. Can simply edit the top-level `for` loop if you want to run inference on other partitions."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"FQX4bIxcKWlf"},"outputs":[{"name":"stderr","output_type":"stream","text":["[                                                                        ]   0%\r"]},{"name":"stdout","output_type":"stream","text":["Testing model: /home/nliang/595project/saved_models/best_now_cross_attetion_epoch6.\n","\tBeginning evaluation...\n","\t\tRunning prediction...\n"]},{"name":"stderr","output_type":"stream","text":["[########################################################################] 100%\n"]},{"name":"stdout","output_type":"stream","text":["\t\tComputing metrics...\n","(18954, 7) (351, 2, 27, 7)\n","\tFinished evaluation in 0:00:43s.\n","\n","PARTITION: test\n","Stories:\n","{\n","  accuracy: \n","    0.8347578347578347,\n","  f1: \n","    0.8347457627118644,\n","  verifiability: \n","    0.1396011396011396,\n","}\n","\n","\n","Conflicts:\n","{\n","  accuracy: \n","    0.9798534798534798,\n","  f1: \n","    0.6900896877242947,\n","}\n","\n","\n","Preconditions:\n","{\n","  accuracy: \n","    0.9963268213268214,\n","  f1: \n","    0.5713604865824486,\n","  accuracy_0: \n","    0.9982589427033871,\n","  f1_0: \n","    0.6049942174328177,\n","  accuracy_1: \n","    0.9994121105232217,\n","  f1_1: \n","    0.6629083995629473,\n","  accuracy_2: \n","    0.9996080736821478,\n","  f1_2: \n","    0.8095247875461741,\n","  accuracy_3: \n","    0.9995779255038514,\n","  f1_3: \n","    0.718131371503082,\n","  accuracy_4: \n","    0.9999246295542592,\n","  f1_4: \n","    0.5237969610270182,\n","  accuracy_5: \n","    0.9896817859780823,\n","  f1_5: \n","    0.5182569235910599,\n","  accuracy_6: \n","    0.9875789505419135,\n","  f1_6: \n","    0.7777753074051472,\n","  accuracy_7: \n","    0.9985152022189059,\n","  f1_7: \n","    0.467540676260697,\n","  accuracy_8: \n","    0.9973695714436455,\n","  f1_8: \n","    0.8015035911824661,\n","  accuracy_9: \n","    0.98707396855545,\n","  f1_9: \n","    0.7132165515790224,\n","  accuracy_10: \n","    0.9981157388564796,\n","  f1_10: \n","    0.6694354417137337,\n","  accuracy_11: \n","    0.9983870724611466,\n","  f1_11: \n","    0.7577641926656155,\n","  accuracy_12: \n","    0.9974675530231086,\n","  f1_12: \n","    0.8326990492261471,\n","  accuracy_13: \n","    0.99852273926348,\n","  f1_13: \n","    0.46383057667269867,\n","  accuracy_14: \n","    0.9975429234688494,\n","  f1_14: \n","    0.657281610900121,\n","  accuracy_15: \n","    0.9969701080812192,\n","  f1_15: \n","    0.5465701352908452,\n","  accuracy_16: \n","    0.9971811453292935,\n","  f1_16: \n","    0.7806562677661736,\n","  accuracy_17: \n","    0.9868403201736535,\n","  f1_17: \n","    0.5900699281939811,\n","  accuracy_18: \n","    0.9996683700387404,\n","  f1_18: \n","    0.8224976989104609,\n","  accuracy_19: \n","    0.9988392951355914,\n","  f1_19: \n","    0.8139188793294734,\n","}\n","\n","\n","Effects:\n","{\n","  accuracy: \n","    0.9961350035424109,\n","  f1: \n","    0.5689143305304266,\n","  accuracy_0: \n","    0.9982589427033871,\n","  f1_0: \n","    0.6074324421722485,\n","  accuracy_1: \n","    0.9993367400774809,\n","  f1_1: \n","    0.7457735289636437,\n","  accuracy_2: \n","    0.9994573327906662,\n","  f1_2: \n","    0.835263130699086,\n","  accuracy_3: \n","    0.9995779255038514,\n","  f1_3: \n","    0.8028456388698982,\n","  accuracy_4: \n","    0.9995779255038514,\n","  f1_4: \n","    0.5244774233883829,\n","  accuracy_5: \n","    0.9896516377997859,\n","  f1_5: \n","    0.5125043878936199,\n","  accuracy_6: \n","    0.9872171724023576,\n","  f1_6: \n","    0.8071463385452696,\n","  accuracy_7: \n","    0.9982740167925354,\n","  f1_7: \n","    0.721642883706321,\n","  accuracy_8: \n","    0.9973092750870529,\n","  f1_8: \n","    0.7985811113670068,\n","  accuracy_9: \n","    0.9867724867724867,\n","  f1_9: \n","    0.7930510100155331,\n","  accuracy_10: \n","    0.9979273127421275,\n","  f1_10: \n","    0.7354514624106664,\n","  accuracy_11: \n","    0.9986207208429431,\n","  f1_11: \n","    0.8055837116537464,\n","  accuracy_12: \n","    0.9983795354165724,\n","  f1_12: \n","    0.786495393260387,\n","  accuracy_13: \n","    0.9973544973544973,\n","  f1_13: \n","    0.5417150268796593,\n","  accuracy_14: \n","    0.9976559791374606,\n","  f1_14: \n","    0.6956957795277608,\n","  accuracy_15: \n","    0.9947994392438837,\n","  f1_15: \n","    0.6198249414857111,\n","  accuracy_16: \n","    0.9973318862207751,\n","  f1_16: \n","    0.8284802327997597,\n","  accuracy_17: \n","    0.9867423385941905,\n","  f1_17: \n","    0.7233386563978962,\n","  accuracy_18: \n","    0.9995553143701292,\n","  f1_18: \n","    0.5528200500732029,\n","  accuracy_19: \n","    0.9988995914921841,\n","  f1_19: \n","    0.5527403897741295,\n","}\n","\n","\n"]}],"source":["importlib.reload(www.model.transformers_ext)\n","import www.model.transformers_ext\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from www.model.eval import evaluate_tiered, save_results, save_preds, list_comparison, add_entity_attribute_labels\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","metrics = [(accuracy_score, 'accuracy'), (precision_score, 'precision'), (recall_score, 'recall'), (f1_score, 'f1')]\n","import numpy as np\n","\n","from www.utils import print_dict\n","\n","print('Testing model: %s.' % probe_model)\n","\n","# May alter this depending on which partition(s) you want to run inference on\n","for p in tiered_dataset:\n","  if p != 'test':\n","    continue\n","\n","  p_dataset = tiered_dataset[p]\n","  p_tensor_dataset = tiered_tensor_dataset[p]\n","  p_sampler = SequentialSampler(p_tensor_dataset)\n","  p_dataloader = DataLoader(p_tensor_dataset, sampler=p_sampler, batch_size=16)\n","  dev_dataset_name = subtask + '_%s_' + p\n","  p_ids = [ex['example_id'] for ex in tiered_dataset[p]]\n","\n","  # Get preds and metrics on this partition\n","  metr_attr, all_pred_atts, all_atts, \\\n","  metr_prec, all_pred_prec, all_prec, \\\n","  metr_eff, all_pred_eff, all_eff, \\\n","  metr_conflicts, all_pred_conflicts, all_conflicts, \\\n","  metr_stories, all_pred_stories, all_stories, explanations = evaluate_tiered(model, p_dataloader, device, [(accuracy_score, 'accuracy'), (f1_score, 'f1')], seg_mode=False, return_explanations=True)\n","  explanations = add_entity_attribute_labels(explanations, tiered_dataset[p], list(att_to_num_classes.keys()))\n","\n","  save_results(metr_attr, probe_model, dev_dataset_name % 'attributes')\n","  save_results(metr_prec, probe_model, dev_dataset_name % 'preconditions')\n","  save_results(metr_eff, probe_model, dev_dataset_name % 'effects')\n","  save_results(metr_conflicts, probe_model, dev_dataset_name % 'conflicts')\n","  save_results(metr_stories, probe_model, dev_dataset_name % 'stories')\n","  save_results(explanations, probe_model, dev_dataset_name % 'explanations')\n","\n","  print('\\nPARTITION: %s' % p)\n","  print('Stories:')\n","  print_dict(metr_stories)\n","  print('Conflicts:')\n","  print_dict(metr_conflicts)\n","  print('Preconditions:')\n","  print_dict(metr_prec)\n","  print('Effects:')\n","  print_dict(metr_eff)"]},{"cell_type":"markdown","metadata":{"id":"ON1UAnbc8OOE"},"source":["#### Add Consistency Metric to Model Results\n","The intermediate conistency metric isn't included in the originally calculated metrics. This block adds the consistency metric to pre-existing model directory based on the tiered predictions. Generates a new `results_cloze_stories_final_[partition].json` file that includes the consistency metric.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1obFel58pd-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 95 consistent preds in dev (versus 37 verifiable)\n","Found 87 consistent preds in test (versus 34 verifiable)\n"]}],"source":["import json\n","import os\n","\n","model_directories = [eval_model_dir]\n","\n","partitions = ['dev', 'test']\n","expl_fname = 'results_cloze_explanations_%s.json'\n","endtask_fname = 'results_cloze_stories_%s.json'\n","endtask_fname_new = 'results_cloze_stories_final_%s.json'\n","for md in model_directories:\n","  for p in partitions:\n","    explanations = json.load(open(os.path.join(DRIVE_PATH, 'saved_models', md, expl_fname % p), 'r'))\n","    endtask_results = json.load(open(os.path.join(DRIVE_PATH, 'saved_models', md, endtask_fname % p), 'r'))\n","\n","    consistent_preds = 0\n","    verifiable_preds = 0\n","    total = 0\n","    for expl in explanations:\n","      if expl['valid_explanation']:\n","        verifiable_preds += 1\n","      if expl['story_pred'] == expl['story_label']:\n","        if len(expl['conflict_pred']) == len(expl['conflict_label']) and expl['conflict_pred'][0] == expl['conflict_label'][0] and expl['conflict_pred'][1] == expl['conflict_label'][1]:\n","          expl['consistent'] = True\n","          consistent_preds += 1\n","        else:\n","          expl['consistent'] = False\n","      total += 1\n","\n","    endtask_results['consistency'] = float(consistent_preds) / total\n","    print('Found %s consistent preds in %s (versus %s verifiable)' % (str(consistent_preds), p, str(verifiable_preds)))\n","    json.dump(explanations, open(os.path.join(DRIVE_PATH, 'saved_models', md, (expl_fname % p).replace('explanations', 'explanations_consistency')), 'w'))\n","    json.dump(endtask_results, open(os.path.join(DRIVE_PATH, 'saved_models', md, endtask_fname_new % p), 'w'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["XL9AU7zTgP9n","qqvj34KhLL0k","xm7qzKnAnbU9","FoY37xIF-oP7","QzFnAVtuUmpQ","lKY0hTEgnQgB","k_0WsycpFMdb","Cz5tcmScJrka","BxIYaEobhR7J","v7VlN2jUwvcC","5ctQweSlAceo","0q-xjfYU_cV8","-fQ6wXQIBdq1","U-BMInyrBdq2","8fRC3cnLBdq3","aWFmGRhznl2T","Rbvpm9irn3qL","RfXiCTA9KPjG","ON1UAnbc8OOE","W7MADzgfvtv3","Msxt3xAhvtv6","rn5Ywwvxvtv6","Ytbj9Uxxvtv7","h9YL5qnRvtv7","Jss8T2xzvtv8","8XhSrDlpI0aH","CADDFieTvtv9","maZGCMvMvtv-","XnWDvQG7vtv-","KxRnuX_fvtv_","apGoBEp_vtv_","ukLFJ-Mfvtv_","XERMrM56vtv_","Egyx9BejvtwC","auYYxFc6vtwC","fYRfQG6LvtwC"],"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
